[
  {
    "objectID": "miniproject2.html",
    "href": "miniproject2.html",
    "title": "miniproject2",
    "section": "",
    "text": "Mini Project 2, Functions\n\nSampling of The Stuff + Model\nThe Stuff + model. A baseball model that I co created that hopes to observe the movement profile of a each players pitches in a vacuum. Then using the observations generates a quantitative model using machine learning to quantify each pitch. However, we are at a roadblock which is limited data. Currently we think our model works, but we only have access to the data of around 20 pitchers. That sample is not large enough in my opinion. I would like to run a randomization test to see if the model is significant. Then test the power of this model, so we can get an idea on how many samples will be needed to get a significant p value.\nFirst the data needed to be downloaded. All of this data came from St. Olaf Baseball practices throughout the fall of 2023 and spring of 2024. Then initial modeling yielded an observed correlation of -.233. A randomization test was ran by shuffling the response variable of FIP (Fielding Independent Pitching)\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nstuff_plus_data &lt;- read.csv(\"stuff.csv\")\n\nstuff_plus_data\n\n   Stuff_Plus       FIP\n1   104.01935  2.472405\n2   105.54333  6.410095\n3   101.36590  8.721174\n4   100.31467  4.823055\n5    95.68439  4.686691\n6    96.68753  6.575580\n7    99.15839  4.326691\n8    98.15778  5.304338\n9   106.05395  5.136691\n10  101.28690  4.343834\n11  101.17884  2.564740\n12   96.89767  5.963287\n13  101.36075  3.423533\n14   96.84147  4.186691\n15   95.59163 13.550331\n16  100.30834  4.767772\n17   91.60392  3.912497\n18   93.45334  6.627867\n\nobserved_model&lt;- lm(FIP~Stuff_Plus, stuff_plus_data)\nsummary(observed_model)\n\n\nCall:\nlm(formula = FIP ~ Stuff_Plus, data = stuff_plus_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6566 -1.5170 -0.4707  0.6313  7.5779 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  20.2771    15.5138   1.307    0.210\nStuff_Plus   -0.1496     0.1563  -0.958    0.353\n\nResidual standard error: 2.544 on 16 degrees of freedom\nMultiple R-squared:  0.0542,    Adjusted R-squared:  -0.004914 \nF-statistic: 0.9169 on 1 and 16 DF,  p-value: 0.3525\n\n#Observed Multiple R^2 .0542\n\nobserved_r2&lt;- .0542\n\nobserved_cor&lt;- cor(stuff_plus_data$Stuff_Plus, stuff_plus_data$FIP)\n\n\nshuffled_cor&lt;- vector(\"double\", 1000)\nfor(i in 1:1000){\nnew_stuff &lt;-stuff_plus_data|&gt;\n  mutate(shuffled_FIP= sample(FIP))\n  \nshuffled_cor[i]&lt;- cor(new_stuff$Stuff_Plus, new_stuff$shuffled_FIP)\n}\n\nThe shuffling of FIP was ran 1000 times to create a random distribution. The graph below displays the distribution of those shuffled samples with the red line representing the observed sample.\n\nnull_world&lt;- tibble(shuffled_cor= shuffled_cor)\nggplot(null_world, aes(x= shuffled_cor))+\n  geom_histogram()+\n  geom_vline(xintercept= observed_cor, color= \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nSince our observed correlations did not fall outside of the range of possible random outcomes, the next step is to find the power of the model. This power calculation will give an idea of how many samples will be needed to determine if the Stuff + model is truly significant.\nFirst a set of parameters were set for the simulation. The parameters represent the intercept, slope, and standard error of the observed model. A function was created to make values of X_FIP, or expected FIP based on simulating the Stuff + model. Values of expected FIP were simulated using a sample of 100 randomized Stuff + models.\nUsing that function, a second function was created to estimate power for a sample size of 100. That simulated value varies but normally falls between .65 and .75. Finally a while loop was created which ran the estimate power function with increasing values of sample size until power reached above the .95 threshold. A power over that threshold represents a sample large enough to make the model significant.\n\nmodel_intercept&lt;- 20.2771\nmodel_slope&lt;- -.1496\nmodel_sd_error &lt;- 2.544\nmin_stuff_plus&lt;- min(stuff_plus_data$Stuff_Plus)\nmax_stuff_plus&lt;- max(stuff_plus_data$Stuff_Plus)\n\nsimulate_expected_fip &lt;- function(n, intercept, slope, sd_error,m,ma) {\n  stuff_plus &lt;- runif(n, min = m, max = ma)  \n  error &lt;- rnorm(n, mean = 0, sd = sd_error) \n  x_fip &lt;- intercept + slope * stuff_plus + error  \n  \n  model &lt;- lm(x_fip ~ stuff_plus) \n  summary(model)$coefficients[2, 4] \n}\n\nsimulate_expected_fip(100,model_intercept,model_slope,model_sd_error,min_stuff_plus,max_stuff_plus)\n\n[1] 0.07655923\n\nestimate_power &lt;- function(n, intercept, slope, sd_error, m, ma, threshold = 0.05, iterations = 1000) {\n  p_values &lt;- replicate(iterations, simulate_expected_fip(n, intercept, slope, sd_error, m, ma))\n  mean(p_values &lt; threshold) \n}\n\nestimate_power(100, model_intercept, model_slope, model_sd_error, min_stuff_plus, max_stuff_plus)\n\n[1] 0.711\n\npower&lt;- 0\n\nn&lt;- 100\n\nwhile(power&lt; .95){\n  power&lt;- estimate_power(n, model_intercept, model_slope, model_sd_error, min_stuff_plus, max_stuff_plus)\n  print(paste(\"Sample size:\", n, \"Power:\", power))\n  \nif (power &lt; .95){\n  n&lt;- n+10 \n}\n}\n\n[1] \"Sample size: 100 Power: 0.663\"\n[1] \"Sample size: 110 Power: 0.738\"\n[1] \"Sample size: 120 Power: 0.736\"\n[1] \"Sample size: 130 Power: 0.796\"\n[1] \"Sample size: 140 Power: 0.822\"\n[1] \"Sample size: 150 Power: 0.848\"\n[1] \"Sample size: 160 Power: 0.878\"\n[1] \"Sample size: 170 Power: 0.888\"\n[1] \"Sample size: 180 Power: 0.904\"\n[1] \"Sample size: 190 Power: 0.918\"\n[1] \"Sample size: 200 Power: 0.932\"\n[1] \"Sample size: 210 Power: 0.937\"\n[1] \"Sample size: 220 Power: 0.942\"\n[1] \"Sample size: 230 Power: 0.95\"\n\n\nThe simulation yields that a sample between 210 and 230 observations should be large enough to make Stuff + significant by itself. Below is a graph displaying an example of how power trends as sample size increases.\n\nsample_table &lt;- tibble(\n  Sample_Size= c(100,110,120,130,140,150,160,170,180,190,200,210,220),\n  Power= c(.661,.709,.763,.788,.815,.844,.848,.886,.894,.918,.934,.942,.954)\n)\n\nggplot(sample_table, aes(x= Sample_Size, y= Power))+\n  geom_point()+\n  geom_hline(yintercept = .95, color= \"red\")+\n  theme_minimal()+\n  labs(title= \"Trend of Power Versus Sample Size\", x= \"Sample Size\", caption= \"Data collected from fall and spring practices of St. Olaf Baseball\")\n\n\n\n\n\n\n\n\nThese groups of simulations showed that the Stuff + model by itself is not significant yet. However, gathering a sample size of roughly 220 observations in the next year of time seems very reasonable. With an increased sample size, and the re running of machine learning algorithms, I am very hopeful that this model will get significantly more accurate. The other very exciting part of this simulation is that Stuff + only makes up a part of a part of the total pitching model, and is the least accurate piece. If all it takes for Stuff + to be significant is just over 200 observation, that makes me excited about how accurate the entire model can get with increased sample size."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Henry Gliedman Webpage",
    "section": "",
    "text": "Image description\n\n\nMathematics Student at St. Olaf College  MSCS Department @ St. Olaf College\n\n\nHigh School Diploma  2021 | Highland Park Senior High\nBA in Math, concentrations in Statistics/Data Science and Engineering 2025 | St. Olaf College"
  },
  {
    "objectID": "index.html#henry-gliedman",
    "href": "index.html#henry-gliedman",
    "title": "Henry Gliedman Webpage",
    "section": "",
    "text": "Image description\n\n\nMathematics Student at St. Olaf College  MSCS Department @ St. Olaf College\n\n\nHigh School Diploma  2021 | Highland Park Senior High\nBA in Math, concentrations in Statistics/Data Science and Engineering 2025 | St. Olaf College"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "maps",
    "section": "",
    "text": "#Maps\nHello this is the Maps page. Here we are going to explore some maps, and how we can infer visual data.\n\nMap 1:\nFirst we are going to import our data into R, and clean the NBA data so it can be joined to our US States data. The first map we are looking at today is a map of the distribution of NBA basketball players in the 48 continental states. First we are going to import our data into R, and clean the NBA data so it can be joined to our US States data. Then we will display the chart.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mdsr)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nnba_data&lt;- read.csv(\"Players.csv\")\n#Edit NBA Data\nnew_NBA&lt;- nba_data|&gt;\n  mutate(birth_state= tolower(birth_state))|&gt;\n  drop_na(birth_state)|&gt;\n  group_by(birth_state)|&gt;\n  count(birth_state)\n\n#States Data\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nus_states &lt;- map_data(\"state\")\n\nus_states|&gt; \n  right_join(new_NBA, by=c(\"region\"= \"birth_state\"))|&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) + \n  geom_polygon(aes(fill = n), color = \"black\")+\n  scale_fill_gradient(low = \"white\", high = \"darkred\", na.value = \"white\", \n                      name = \"NBA Players\") +\n  labs(title = \"NBA Players by US State\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"right\", panel.grid = element_blank(), axis.text = element_blank(), axis.title = element_blank())\n\n\n\n\n\n\n\n\nAs we can see the map shows that a large amount of NBA talent comes from California and New York. With an exception in California it is also true that a large portion of NBA players come from states East of the Mississippi. This makes a lot of sense because basketball is culturally a much larger deal on the east coast and in Indiana and Ohio\n\n\nMap 2 Gerrymandering in Wisconsin\n\nlibrary(fec16)\n\ndistrict_elections &lt;- results_house |&gt;\n  mutate(district = parse_number(district_id)) |&gt;\n  group_by(state, district) |&gt;\n  summarize(\n    N = n(), \n    total_votes = sum(general_votes, na.rm = TRUE),\n    d_votes = sum(ifelse(party == \"D\", general_votes, 0), na.rm = TRUE),\n    r_votes = sum(ifelse(party == \"R\", general_votes, 0), na.rm = TRUE),\n    .groups = \"drop\" ) |&gt;\n  mutate(\n    other_votes = total_votes - d_votes - r_votes,\n    r_prop = r_votes / total_votes,  \n    winner = ifelse(r_votes &gt; d_votes, \"Republican\", \"Democrat\")\n  )\n\nwis_results &lt;- district_elections |&gt;\n  filter(state == \"WI\")\nwis_results |&gt;                  \n  select(-state)\n\n# A tibble: 8 × 8\n  district     N total_votes d_votes r_votes other_votes r_prop winner  \n     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1        1     7      353990       0       0      353990      0 Democrat\n2        2     2      397581       0       0      397581      0 Democrat\n3        3     2      257401       0       0      257401      0 Democrat\n4        4     4      285858       0       0      285858      0 Democrat\n5        5     3      390507       0       0      390507      0 Democrat\n6        6     4      356935       0       0      356935      0 Democrat\n7        7     4      362061       0       0      362061      0 Democrat\n8        8     4      363574       0       0      363574      0 Democrat\n\n\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\nsrc &lt;- \"http://cdmaps.polisci.ucla.edu/shp/districts113.zip\"\nlcl_zip &lt;- fs::path(tempdir(), \"districts113.zip\")\ndownload.file(src, destfile = lcl_zip)\nlcl_districts &lt;- fs::path(tempdir(), \"districts113\")\nunzip(lcl_zip, exdir = lcl_districts)\ndsn_districts &lt;- fs::path(lcl_districts, \"districtShapes\")\n\n# You can also downloaded zip file and uploaded it into R, but this uses a ton of space!\n# dsn_districts &lt;- fs::path(\"Data/districtShapes\")\n\n# read shapefiles into R as an sf object\nst_layers(dsn_districts)\n\nDriver: ESRI Shapefile \nAvailable layers:\n    layer_name geometry_type features fields crs_name\n1 districts113       Polygon      436     15    NAD83\n\n# be able to read as a data frame as well\ndistricts &lt;- st_read(dsn_districts, layer = \"districts113\") |&gt;\n  mutate(DISTRICT = parse_number(as.character(DISTRICT)))\n\nReading layer `districts113' from data source \n  `C:\\Users\\henry\\AppData\\Local\\Temp\\RtmpiQP0vx\\districts113\\districtShapes' \n  using driver `ESRI Shapefile'\nSimple feature collection with 436 features and 15 fields (with 1 geometry empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1473 ymin: 18.91383 xmax: 179.7785 ymax: 71.35256\nGeodetic CRS:  NAD83\n\n# create basic plot with NC congressional districts\nwis_shp &lt;- districts |&gt;\n  filter(STATENAME == \"Wisconsin\")\n\n\n# Append election results to geospatial data\nwis_merged &lt;- wis_shp |&gt;\n  st_transform(4326) |&gt;\n  inner_join(wis_results, by = c(\"DISTRICT\" = \"district\"))\n\n\n\n#wis &lt;- ggplot(data = wis_merged, aes(fill = winner)) +\n # annotation_map_tile(zoom = 6, type = \"osm\", progress = \"none\") + \n#  geom_sf(alpha = 0.5) +\n#  scale_fill_manual(\"Winner\", values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) + \n # theme_void()\n#wis\n\n\n# A leaflet map can allow us to zoom in and see where major cities fit, etc.\nlibrary(leaflet)\npal &lt;- colorNumeric(palette = \"RdBu\", domain = c(0, 1))\n\nleaflet_wis &lt;- leaflet(wis_merged) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    weight = 1, fillOpacity = 0.7, \n    color = ~pal(1 - r_prop),   # so red association with Reps\n    popup = ~paste(\"District\", DISTRICT, \"&lt;/br&gt;\", round(r_prop, 4))\n  ) |&gt;                          # popups show prop Republican\n  setView(lng = -90, lat = 45, zoom = 7)\nleaflet_wis\n\n\n\n\n\nThis map may not be properly displaying the data. Sometimes when the website is loaded the leaflet data doesn’t properly transfer, but it should be seen from the code that the democrats handily win districts 2,3, and 4. While the Republicans win the other 5 districts. Based on the data saying that district 2,3, and 4 were all won by large margins, and districts 1,7, and 8 were all close elections, I would conclude that there is some level of gerrymandering going on in Wisconsin."
  }
]